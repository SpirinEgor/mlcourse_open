{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению. Сессия № 3\n",
    "<center> Автор материала: Павел Нестеров (@mephistopheies).\n",
    "\n",
    "Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашняя работа №4\n",
    "## <center> Логистическая регрессия в задаче тегирования вопросов StackOverflow\n",
    "\n",
    "**Надо вывести формулы, где это просится (да, ручка и бумажка), заполнить код в клетках и выбрать ответы в [веб-форме](https://docs.google.com/forms/d/100c3Ek94UL-VRwXrN4lxCSnGjfJrl6Gc96G21DNCh4w).**\n",
    "\n",
    "## 0. Описание задачи\n",
    "\n",
    "В этой домашней работе мы с вами изучим и запрограммируем модель для прогнозирования тегов по тексту вопроса на базе многоклассовой логистической регрессии. В отличие от обычной постановки задачи классификации (multiclass), в данном случае один пример может принадлежать одновременно к нескольким классам (multilabel). Мы будем реализовывать онлайн-версию алгоритма multilabel-классификации.\n",
    "\n",
    "Мы будем использовать небольшую выборку из протеггированных вопросов с сайта StackOverflow размером в 125 тысяч примеров (около 150 Мб, скачайте по [этой](https://drive.google.com/open?id=0B4bl7YMqDnViYVo0V2FubFVhMFE) ссылке).\n",
    "\n",
    "PS: Можно показать, что такая реализация совсем не эффективная и проще было бы использовать векторизированные вычисления. Для данного датасета так и есть. Но на самом деле подобные реализации используются в жизни, но естественно, написаны они не на Python. Например, в онлайн-моделях прогнозирования [CTR](https://en.wikipedia.org/wiki/Click-through_rate) юзеру показывается баннер, затем в зависимости от наличия клика происходит обновление параметров модели. В реальной жизни параметров модели может быть несколько сотен миллионов, а у юзера из этих ста миллионов от силы сто или тысяча параметров отличны от нуля, векторизировать такие вычисления не очень эффективно. Обычно все это хранится в огромных кластерах в in-memory базах данных, а обработка пользователей происходит распределенно.\n",
    "\n",
    "PS2:\n",
    "- в процессе решения домашней работы вам придется работать с текстом, и у вас может возникнуть желание сделать очевидный препроцессинг, например привести все слова в нижний регистр, в-общем **этого делать не нужно, если не оговорено заранее в задании**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n"
     ]
    }
   ],
   "source": [
    "#!pip install watermark\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем версии используемых библиотек. Совпадут ли ответы в случае других версий - не гарантируется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.6.3\n",
      "IPython 6.2.1\n",
      "\n",
      "numpy 1.14.1\n",
      "scipy 1.0.0\n",
      "pandas 0.22.0\n",
      "matplotlib 2.2.0\n",
      "sklearn 0.19.1\n",
      "\n",
      "compiler   : GCC 7.2.0\n",
      "system     : Linux\n",
      "release    : 4.15.9-041509-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "Git hash   : 690a9f4f36d8ec18120934c8f1bc9cd3cefbab13\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -m -p numpy,scipy,pandas,matplotlib,sklearn -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "# поменяйте на свой путь\n",
    "DS_FILE_NAME = './stackoverflow_sample_125k.tsv'\n",
    "TAGS_FILE_NAME = './top10_tags.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'php', 'html', 'jquery', 'javascript', 'ios', 'c#', 'python', 'c++', 'android', 'java'}\n"
     ]
    }
   ],
   "source": [
    "top_tags = []\n",
    "with open(TAGS_FILE_NAME, 'r') as f:\n",
    "    for line in f:\n",
    "        top_tags.append(line.strip())\n",
    "top_tags = set(top_tags)\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Многоклассовая логистическая регрессия\n",
    "\n",
    "Вспомним, как получается логистическая регрессия для двух классов $\\left\\{0, 1\\right\\}$, вероятность принадлежности объекта к классу $1$ выписывается по теореме Байеса:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = 1 \\mid \\textbf{x}\\right) &=& \\dfrac{p\\left(\\textbf{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\textbf{x} \\mid c = 1\\right)p\\left(c = 1\\right) + p\\left(\\textbf{x} \\mid c = 0\\right)p\\left(c = 0\\right)} \\\\\n",
    "&=& \\dfrac{1}{1 + e^{-a}} = \\sigma\\left(a\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\textbf{x}$ – вектор признаков объекта\n",
    "- $\\sigma$ – обозначение функции логистического сигмоида при скалярном аргументе\n",
    "- $a = \\log \\frac{p\\left(\\textbf{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\textbf{x} \\mid c = 0\\right)p\\left(c = 0\\right)} = \\sum_{i=0}^M w_i x_i$ – это отношение мы моделируем линейной функцией от признаков объекта и параметров модели\n",
    "\n",
    "Данное выражение легко обобщить до множества из $K$ классов, изменится только знаменатель в формуле Байеса. Запишем вероятность принадлежности объекта к классу $k$:\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = k \\mid \\textbf{x}\\right) &=& \\dfrac{p\\left(\\textbf{x} \\mid c = k\\right)p\\left(c = k\\right)}{\\sum_{i=1}^K p\\left(\\textbf{x} \\mid c = i\\right)p\\left(c = i\\right)} \\\\\n",
    "&=& \\dfrac{e^{z_k}}{\\sum_{i=1}^{K}e^{z_i}} = \\sigma_k\\left(\\textbf{z}\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\sigma_k$ – обозначение функции softmax при векторном аргументе\n",
    "- $z_k = \\log p\\left(\\textbf{x} \\mid c = k\\right)p\\left(c = k\\right) = \\sum_{i=0}^M w_{ki} x_i$ – это выражение моделируется линейной функцией от признаков объекта и параметров модели для класса $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для моделирования полного правдоподобия примера мы используем [категориальное распределение](https://en.wikipedia.org/wiki/Categorical_distribution), а лучше его логарифм (для удобства):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\mathcal{L} = \\log p\\left({\\textbf{x}, y}\\right) &=& \\log \\prod_{i=1}^K \\sigma_i\\left(\\textbf{z}\\right)^{y_i} \\\\\n",
    "&=& \\sum_{i=1}^K y_i \\log \\sigma_i\\left(\\textbf{z}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Получается хорошо знакомая нам функция [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) (если домножить на $-1$). Правдоподобие нужно максимизировать, а, соответственно, перекрестную энтропию нужно минимизировать. Продифференцировав по параметрам модели, мы _легко_ получим правила обновления весов для градиентного спуска, **проделайте этот вывод, если вы его не делали** (если вы вдруг сдались, то на [этом](https://www.youtube.com/watch?v=-WiR16raQf4) видео есть разбор вывода, понимание этого вам понадобится для дальнейшего выполнения задания; если предпочитаете текст, то и он есть [тут](https://www.ics.uci.edu/~pjsadows/notes.pdf) и [тут](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} &=& x_m \\left(y_k - \\sigma_k\\left(\\textbf{z}\\right)\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "В стандартной формулировке получается, что вектор $\\left(\\sigma_1, \\sigma_2, \\ldots, \\sigma_K\\right)$ образует дискретное вероятностное распределение, т.е. $\\sum_{i=1}^K \\sigma_i = 1$. Но в нашей постановке задачи каждый пример может иметь несколько тегов или одновременно принадлежать к нескольким классам. Для этого мы немного изменим модель:\n",
    "- будем считать, что все теги независимы друг от друга, т.е. каждый исход – это логистическая регрессия на два класса (либо есть тег, либо его нет), тогда вероятность наличия тега у примера запишется следующим образом (каждый тег/класс как и в многоклассовой логрегрессии имеет свой набор параметров):\n",
    "$$\\large p\\left(\\text{tag}_k \\mid \\textbf{x}\\right) = \\sigma\\left(z_k\\right) = \\sigma\\left(\\sum_{i=1}^M w_{ki} x^i \\right)$$\n",
    "- наличие каждого тега мы будем моделировать с помощью <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\">распределения Бернулли</a>\n",
    "\n",
    "<font color=\"red\">Вопрос 1.</font> Ваше первое задание –  записать упрощенное выражение логарифма правдоподобия примера с признаками $\\textbf{x}$. Как правило, многие алгоритмы оптимизации имеют интерфейс для минимизации функции, мы последуем этой же традиции и домножим полученное выражение на $-1$, а во второй части выведем формулы для минимизации полученного выражения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. $\\large -\\mathcal{L} = -\\sum_{i=1}^M y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "2. $\\large -\\mathcal{L} = -\\sum_{i=1}^K y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "3. $\\large -\\mathcal{L} = -\\sum_{i=1}^K z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$\n",
    "4. $\\large -\\mathcal{L} = -\\sum_{i=1}^M z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Вывод формулы обновления весов\n",
    "\n",
    "<font color=\"red\">Вопрос 2.</font> В качестве второго задания вам предоставляется возможность вывести формулу градиента для $-\\mathcal{L}$. Какой вид она будет иметь?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(\\sigma\\left(z_k\\right) - y_k\\right)$\n",
    "2. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(y_k - \\sigma\\left(z_k\\right)\\right)$\n",
    "3. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(\\sigma\\left(z_k\\right)x_m - y_k\\right)$\n",
    "4. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(y_k - \\sigma\\left(z_k\\right)x_m\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Реализация базовой модели\n",
    "\n",
    "Вам предлагается каркас класса модели, разберите его внимательно, обращайте внимание на комментарии. Затем заполните пропуски, запустите полученную модель и ответьте на проверочный вопрос.\n",
    "\n",
    "Как вы могли уже заметить, при обновлении веса $w_{km}$ используется значение признака $x_m$, который равен $0$, если слова с индексом $m$ нет в предложении, и больше нуля, если такое слово есть. В нашем случае, чтобы не пересчитывать [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) самим или с помощью [sklearn.feature_extraction.text.CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer), мы будем идти по словам предложения в порядке их следования. Если какое-то слово встречается несколько раз, то мы добавляем его в аккумулятор со своим весом. В итоге получится то же самое, как если сначала посчитать количество одинаковых слов и домножить на соответствующий вес. Соответственно, при вычислении линейной комбинации $z$ весов модели и признаков примера необходимо учитывать только ненулевые признаки объекта.\n",
    "\n",
    "Подсказка:\n",
    "- если реализовывать вычисление сигмоида так же, как в формуле, то при большом отрицательном значении $z$ вычисление $e^{-z}$ превратится в очень большое число, которое вылетит за допустимые пределы\n",
    "- в то же время $e^{-z}$ от большого положительного $z$ будет нулем\n",
    "- воспользуйтесь свойствами функции $\\sigma$ для того, чтобы пофиксить эту ошибку и реализовать $\\sigma$ без риска overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sigma = expit(z)\n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    log_arg = sigma\n",
    "                    if log_arg > 1 - tolerance:\n",
    "                        log_arg = 1 - tolerance\n",
    "                    elif log_arg < tolerance:\n",
    "                        log_arg = tolerance\n",
    "                    sample_loss += -y * np.log(log_arg) - (1 - y) * np.log(1 - log_arg)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = (y - sigma)\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0473191eba4dd0821575c74c719431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# создадим эксемпляр модели и пройдемся по датасету\n",
    "model = LogRegressor()\n",
    "model.iterate_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, действительно ли значение отрицательного логарифмического правдоподобия уменьшалось. Так как мы используем стохастический градентный спуск, не стоит ожидать плавного падения функции ошибки. Мы воспользуемся скользящим средним с окном в 10 тысяч примеров, чтобы хоть как-то сгладить график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the loss function on the last 10k train samples: 19.40\n"
     ]
    }
   ],
   "source": [
    "print('Mean of the loss function on the last 10k train samples: %0.2f' % np.mean(model._loss[-35000:-25000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 3.</font>\n",
    "Вычислите среднее значение функции стоимости на последних 10 000 примеров тренировочного набора, к какому из значений ваш ответ ближе всего?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 17.54\n",
    "2. 18.64\n",
    "3. 19.74\n",
    "4. 20.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Тестирование модели\n",
    "\n",
    "В базовой модели первые 100 000 строк используются для обучения, а оставшиеся – для тестирования. Как вы можете заметить, значение отрицательного логарифмического правдоподобия не очень информативно, хоть и позволяет сравнивать разные модели. В качестве четвертого задания вам необходимо модифицировать базовую модель таким образом, чтобы метод `iterate_file` возвращал значение _точности_ на тестовой части набора данных. \n",
    "\n",
    "Точность определим следующим образом:\n",
    "- считаем, что тег у вопроса присутствует, если спрогнозированная вероятность тега больше 0.9\n",
    "- точность одного примера расчитывается как [коэффициент Жаккара](https://ru.wikipedia.org/wiki/Коэффициент_Жаккара) между множеством настоящих тегов и предсказанных моделью\n",
    "  - например, если у примера настоящие теги ['html', 'jquery'], а по версии модели ['ios', 'html', 'java'], то коэффициент Жаккара будет равен |['html', 'jquery'] $\\cap$ ['ios', 'html', 'java']| / |['html', 'jquery'] $\\cup$ ['ios', 'html', 'java']| = |['html']| / |['jquery', 'ios', 'html', 'java']| = 1/4\n",
    "- метод `iterate_file` возвращает **среднюю** точность на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь\n",
    "class LogRegressor(LogRegressor):\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        acc = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                pred_tags = set()\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sigma = expit(z)\n",
    "                \n",
    "                    if n > top_n_train and sigma > 0.9:\n",
    "                        pred_tags.add(tag)\n",
    "        \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    log_arg = sigma\n",
    "                    if log_arg > 1 - tolerance:\n",
    "                        log_arg = 1 - tolerance\n",
    "                    elif log_arg < tolerance:\n",
    "                        log_arg = tolerance\n",
    "                    sample_loss += -y * np.log(log_arg) - (1 - y) * np.log(1 - log_arg)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = (y - sigma)\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "                acc += len(pred_tags & tags) / len(pred_tags | tags)\n",
    "        return acc / (n - top_n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334aaa3f3e5e4244971372c9e3b67e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.58\n"
     ]
    }
   ],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "# выведем полученное значение с точностью до двух знаков\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 4.</font> К какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.39\n",
    "2. 0.49\n",
    "3. 0.59\n",
    "4. 0.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. $L_2$-регуляризация\n",
    "\n",
    "В качестве пятого задания вам необходимо добавить в класс `LogRegressor` поддержку $L_2$-регуляризации. В методе `iterate_file` должен появиться параметр `lmbda=0.01` со значением по умолчанию. С учетом регуляризации новая функция стоимости примет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(\\textbf W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\frac{\\lambda}{2} \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2\n",
    "\\end{array}$$\n",
    "\n",
    "Градиент первого члена суммы мы уже вывели, а для второго он имеет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial}{\\partial w_{ki}} \\frac{\\lambda}{2} R\\left(\\textbf W\\right) &=& \\lambda w_{ki}\n",
    "\\end{array}$$\n",
    "\n",
    "Если мы на каждом примере будем делать честное обновление всех весов, то все очень замедлится, ведь нам придется на каждой итерации пробегать по всем словам словаря. В ущерб теоретической корректности мы используем грязный трюк: будем регуляризировать только те слова, которые присутствуют в текущем предложении. Не забывайте, что смещение (bias) не регуляризируется. `sample_loss` тоже должен остаться без изменений.\n",
    "\n",
    "Замечание:\n",
    "- не забудьте, что нужно учитывать регуляризацию слова в градиентном шаге только один раз\n",
    "- условимся, что учитываем регуляризацию только при первой встрече слова\n",
    "- если бы мы считали сначала bag-of-words, то мы бы в цикле шли по уникальным словам, но т.к. мы этого не делаем, приходится выкручиваться (еще одна жертва богу online-моделей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь\n",
    "class LogRegressor(LogRegressor):\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16,\n",
    "                     lmbda = 0.01):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        acc = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                pred_tags = set()\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    used_word = set()\n",
    "                \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sigma = expit(z)\n",
    "                \n",
    "                    if n > top_n_train and sigma > 0.9:\n",
    "                        pred_tags.add(tag)\n",
    "        \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    log_arg = sigma\n",
    "                    if log_arg > 1 - tolerance:\n",
    "                        log_arg = 1 - tolerance\n",
    "                    elif log_arg < tolerance:\n",
    "                        log_arg = tolerance\n",
    "                    sample_loss += -y * np.log(log_arg) - (1 - y) * np.log(1 - log_arg)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = (y - sigma)\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            if word not in used_word:\n",
    "                                used_word.add(word) \n",
    "                                self._w[tag][self._vocab[word]] -= \\\n",
    "                                    learning_rate * lmbda * self._w[tag][self._vocab[word]]\n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "                acc += len(pred_tags & tags) / len(pred_tags | tags)\n",
    "        return acc / (n - top_n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa80d97cddfe4dddb2a07660c918fc9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.51\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 5.</font> К какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.3\n",
    "2. 0.35\n",
    "3. 0.4\n",
    "4. 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ElasticNet регуляризация, вывод\n",
    "Помимо $L_2$ регуляризации, часто используется $L_1$ регуляризация.\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(\\textbf W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right|\n",
    "\\end{array}$$\n",
    "\n",
    "Если линейно объединить $L_1$ и $L_2$ регуляризацию, то полученный тип регуляризации называется ElasticNet:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\lambda R\\left(\\textbf W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\left(\\gamma \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2 + \\left(1 - \\gamma\\right) \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right| \\right)\n",
    "\\end{array}$$\n",
    "- где $\\gamma \\in \\left[0, 1\\right]$\n",
    "\n",
    "В качестве шестого вопроса вам предлагается вывести формулу градиента ElasticNet регуляризации (не учитывая $-\\mathcal{L}$). \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(\\textbf W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) w_{ki}\\right)$ \n",
    "2. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(\\textbf W\\right) = \\lambda \\left(2 \\gamma \\left|w_{ki}\\right| + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "3. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(\\textbf W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "4. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(\\textbf W\\right) = \\lambda \\left(\\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Регуляризация ElasticNet , реализация\n",
    "\n",
    "В качестве седьмой задачи вам предлается изменить класс `LogRegressor` таким образом, чтобы метод `iterate_file` принимал два параметра со значениями по умолчанию `lmbda=0.0002` и `gamma=0.1`. Сделайте один проход по датасету с включенной `ElasticNet`-регуляризацией и заданными значениями по умолчанию и ответьте на вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь\n",
    "class LogRegressor(LogRegressor):\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16,\n",
    "                     lmbda=0.0002,\n",
    "                     gamma=0.1):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        acc = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                pred_tags = set()\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    used_word = set()\n",
    "        \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sigma = expit(z)\n",
    "                \n",
    "                    if n > top_n_train and sigma > 0.9:\n",
    "                        pred_tags.add(tag)\n",
    "        \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    log_arg = sigma\n",
    "                    if log_arg > 1 - tolerance:\n",
    "                        log_arg = 1 - tolerance\n",
    "                    elif log_arg < tolerance:\n",
    "                        log_arg = tolerance\n",
    "                    sample_loss += -y * np.log(log_arg) - (1 - y) * np.log(1 - log_arg)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = (y - sigma)\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            if word not in used_word:\n",
    "                                used_word.add(word)    \n",
    "                                self._w[tag][self._vocab[word]] -= \\\n",
    "                                    learning_rate * lmbda * (2 * gamma * self._w[tag][self._vocab[word]] +\\\n",
    "                                                             (1 - gamma) * np.sign(self._w[tag][self._vocab[word]]))\n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "                acc += len(pred_tags & tags) / len(pred_tags | tags)\n",
    "        return acc / (n - top_n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13d8cb288e7423fb939dd3d5cc25420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.58\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 7.</font> К какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.59\n",
    "2. 0.69\n",
    "3. 0.79\n",
    "4. 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Самые важные слова для тега\n",
    "\n",
    "Прелесть линейных моделей в том, что они легко интерпретируемы. Вам предлагается вычислить, какие слова вносят наибольший вклад в вероятность появления каждого из тегов. А затем ответьте на контрольный вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html : 3, html, br, amp, nav\n",
      "c# : writeline, binding, textblock, linq, net\n",
      "php : php, x5c, _post, 125, echo\n",
      "javascript : javascript, x20, 3, 125, x30\n",
      "android : android, activity, art, imgsrv, 29297\n",
      "python : python, def, py, np, django\n",
      "ios : ios, nsstring, dylib, nil, xcode\n",
      "c++ : avrf, c++, cout, std, boost\n",
      "jquery : jquery, ready, ajax, span, val\n",
      "java : println, spring, servlet, 176, bean\n"
     ]
    }
   ],
   "source": [
    "model._vocab_inv = dict([(v, k) for (k, v) in model._vocab.items()])\n",
    "\n",
    "for tag in model._tags:\n",
    "    print(tag, ':', ', '.join([model._vocab_inv[k] for (k, v) in \n",
    "                               sorted(model._w[tag].items(), \n",
    "                                      key=lambda t: t[1], \n",
    "                                      reverse=True)[:5]]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 8.</font> Для многих тегов наличие самого тега в предложении является важным сигналом, у многих сам тег является самым сильным сигналом, что не удивительно. Для каких из тегов само название тега не входит в топ-5 самых важных?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. c# \n",
    "2. javascript\n",
    "3. jquery\n",
    "4. android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. Сокращаем размер словаря\n",
    "Сейчас количество слов в словаре – 519290, если бы это была выборка из 10 миллионов вопросов с сайта StackOverflow, то размер словаря был бы миллионов 10. Регуляризировать модель можно не только изящно математически, но и топорно, например, ограничить размер словаря. Вам предоставляется возможность внести следующие изменения в класс `LogRegressor`:\n",
    "- добавить в метод `iterate_file` еще один аргумент со значением по умолчанию `update_vocab=True`\n",
    "- при `update_vocab=True` разрешать добавлять слова в словарь в режиме обучения\n",
    "- при `update_vocab=False` игнорировать слова не из словаря\n",
    "- добавить в класс метод `filter_vocab(n=10000)`, который оставит в словаре только топ-n самых популярных слов, используя данные из ``train``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sign(x):\n",
    "    if x >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь\n",
    "class LogRegressor(LogRegressor):\n",
    "    def __init__(self, tags=top_tags): \n",
    "        self._vocab = {}\n",
    "        self._vocab_count = defaultdict(int)\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        self._b = dict([(t, 0) for t in tags]) \n",
    "        self._tags = set(tags)\n",
    "        \n",
    "        \n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16,\n",
    "                     lmbda=0.0002,\n",
    "                     gamma=0.1,\n",
    "                     update_vocab=True):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        acc = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                pred_tags = set()\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    used_word = set()\n",
    "        \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab and update_vocab == True:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        if word in self._vocab:\n",
    "                            z += self._w[tag][self._vocab[word]]\n",
    "                            self._vocab_count[word] += 1\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sigma = expit(z)\n",
    "                \n",
    "                    if n > top_n_train and sigma > 0.9:\n",
    "                        pred_tags.add(tag)\n",
    "        \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    log_arg = sigma\n",
    "                    if log_arg > 1 - tolerance:\n",
    "                        log_arg = 1 - tolerance\n",
    "                    elif log_arg < tolerance:\n",
    "                        log_arg = tolerance\n",
    "                    sample_loss += -y * np.log(log_arg) - (1 - y) * np.log(1 - log_arg)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = (y - sigma)\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:\n",
    "                            if word in self._vocab:\n",
    "                                if word not in used_word:\n",
    "                                    used_word.add(word)    \n",
    "                                    self._w[tag][self._vocab[word]] -= \\\n",
    "                                        learning_rate * lmbda * (2 * gamma * self._w[tag][self._vocab[word]] +\\\n",
    "                                                                 (1 - gamma) * sign(self._w[tag][self._vocab[word]]))\n",
    "                                self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "                acc += len(pred_tags & tags) / len(pred_tags | tags)\n",
    "        return acc / (n - top_n_train)\n",
    "    \n",
    "    def filter_vocab(self, n=10000):\n",
    "        words = [k for (k, v) in sorted(self._vocab_count.items(), key=lambda t: t[1], reverse=True)]\n",
    "        self._vocab = dict([(word, self._vocab[word]) for word in words[:n]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1145a3c5920249358b5773db2297f88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f201736c6e5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%0.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m25000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrolling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-e22ccc0883e0>\u001b[0m in \u001b[0;36miterate_file\u001b[0;34m(self, fname, top_n_train, total, learning_rate, tolerance, lmbda, gamma, update_vocab)\u001b[0m\n\u001b[1;32m     94\u001b[0m                                 \u001b[0mused_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m                                     \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlmbda\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m                                                             \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdLdw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdLdw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# оставим только топ 10 000 слов\n",
    "model.filter_vocab(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# сделаем еще одну итерацию по датасету, уменьшив скорость обучения в 10 раз\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 9.</font> К какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.48\n",
    "2. 0.58\n",
    "3. 0.68\n",
    "4. 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Прогнозирование тегов для новых вопросов\n",
    "\n",
    "В завершение этого задания вам предлагается реализовать метод `predict_proba`, который принимает строку, содержащую вопрос, а возвращает список предсказанных тегов вопроса с их вероятностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogRegressor(LogRegressor):\n",
    "    def predict_proba(self, s):\n",
    "        sentence = s.split(' ')\n",
    "        pred_tags = dict()\n",
    "        for tag in self._tags:\n",
    "            z = self._b[tag]\n",
    "            for word in sentence:\n",
    "                if word in self._vocab:\n",
    "                    z += self._w[tag][self._vocab[word]]\n",
    "            sigma = expit(z)\n",
    "            pred_tags[tag] = sigma\n",
    "        return pred_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432f51f89627461887454b7620d6aa88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/spirin_egor/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/spirin_egor/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/spirin_egor/anaconda3/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.58\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde452a152ef4c7f971b2e8d5fa23291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.69\n"
     ]
    }
   ],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "model.filter_vocab(n=10000)\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = (\"I want to improve my coding skills, so I have planned write \" +\n",
    "            \"a Mobile Application.need to choose between Apple's iOS or Google's Android.\" +\n",
    "            \" my background: I have done basic programming in .Net,C/C++,Python and PHP \" +\n",
    "            \"in college, so got OOP concepts covered. about my skill level, I just know \" +\n",
    "            \"concepts and basic syntax. But can't write complex applications, if asked :(\" +\n",
    "            \" So decided to hone my skills, And I wanted to know which is easier to \" +\n",
    "            \"learn for a programming n00b. A) iOS which uses Objective C B) Android \" + \n",
    "            \"which uses Java. I want to decide based on difficulty \" + \n",
    "            \"level\").lower().replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ios', 0.9999999999999989),\n",
       " ('php', 0.9999314938869799),\n",
       " ('android', 0.00012388195283082957),\n",
       " ('java', 5.841382256627379e-19),\n",
       " ('c++', 1.672944115605432e-19),\n",
       " ('javascript', 8.606386011427552e-25),\n",
       " ('html', 4.699452215431928e-32),\n",
       " ('python', 1.9905707928852235e-35),\n",
       " ('c#', 2.7188121229124758e-36),\n",
       " ('jquery', 2.6258264964029814e-45)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(model.predict_proba(sentence).items(), \n",
    "       key=lambda t: t[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 10.</font> Отметьте все теги, ассоциирующиеся с данным вопросом, если порог принятия равен $0.9$. То есть считаем, что вопросу надо поставить некоторый тег, если вероятность его появления, предсказанная моделью, больше или равна 0.9. \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. android\n",
    "2. ios\n",
    "3. php\n",
    "4. java"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
