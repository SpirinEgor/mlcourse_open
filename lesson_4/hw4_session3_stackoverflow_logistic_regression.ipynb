{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению. Сессия № 3\n",
    "<center> Автор материала: Павел Нестеров (@mephistopheies).\n",
    "\n",
    "Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашняя работа №4\n",
    "## <center> Логистическая регрессия в задаче тегирования вопросов StackOverflow\n",
    "\n",
    "**Надо вывести формулы, где это просится (да, ручка и бумажка), заполнить код в клетках и выбрать ответы в [веб-форме](https://docs.google.com/forms/d/100c3Ek94UL-VRwXrN4lxCSnGjfJrl6Gc96G21DNCh4w).**\n",
    "\n",
    "## 0. Описание задачи\n",
    "\n",
    "В этой домашней работе мы с вами изучим и запрограммируем модель для прогнозирования тегов по тексту вопроса на базе многоклассовой логистической регрессии. В отличие от обычной постановки задачи классификации (multiclass), в данном случае один пример может принадлежать одновременно к нескольким классам (multilabel). Мы будем реализовывать онлайн-версию алгоритма multilabel-классификации.\n",
    "\n",
    "Мы будем использовать небольшую выборку из протеггированных вопросов с сайта StackOverflow размером в 125 тысяч примеров (около 150 Мб, скачайте по [этой](https://drive.google.com/open?id=0B4bl7YMqDnViYVo0V2FubFVhMFE) ссылке).\n",
    "\n",
    "PS: Можно показать, что такая реализация совсем не эффективная и проще было бы использовать векторизированные вычисления. Для данного датасета так и есть. Но на самом деле подобные реализации используются в жизни, но естественно, написаны они не на Python. Например, в онлайн-моделях прогнозирования [CTR](https://en.wikipedia.org/wiki/Click-through_rate) юзеру показывается баннер, затем в зависимости от наличия клика происходит обновление параметров модели. В реальной жизни параметров модели может быть несколько сотен миллионов, а у юзера из этих ста миллионов от силы сто или тысяча параметров отличны от нуля, векторизировать такие вычисления не очень эффективно. Обычно все это хранится в огромных кластерах в in-memory базах данных, а обработка пользователей происходит распределенно.\n",
    "\n",
    "PS2:\n",
    "- в процессе решения домашней работы вам придется работать с текстом, и у вас может возникнуть желание сделать очевидный препроцессинг, например привести все слова в нижний регистр, в-общем **этого делать не нужно, если не оговорено заранее в задании**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install watermark\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем версии используемых библиотек. Совпадут ли ответы в случае других версий - не гарантируется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%watermark -v -m -p numpy,scipy,pandas,matplotlib,sklearn -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%watermark -v -m -p numpy,scipy,pandas,matplotlib,sklearn -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "# поменяйте на свой путь\n",
    "DS_FILE_NAME = './stackoverflow_sample_125k.tsv'\n",
    "TAGS_FILE_NAME = './top10_tags.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ios', 'jquery', 'python', 'c#', 'html', 'php', 'c++', 'android', 'java', 'javascript'}\n"
     ]
    }
   ],
   "source": [
    "top_tags = []\n",
    "with open(TAGS_FILE_NAME, 'r') as f:\n",
    "    for line in f:\n",
    "        top_tags.append(line.strip())\n",
    "top_tags = set(top_tags)\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Многоклассовая логистическая регрессия\n",
    "\n",
    "Вспомним, как получается логистическая регрессия для двух классов $\\left\\{0, 1\\right\\}$, вероятность принадлежности объекта к классу $1$ выписывается по теореме Байеса:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = 1 \\mid \\textbf{x}\\right) &=& \\dfrac{p\\left(\\textbf{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\textbf{x} \\mid c = 1\\right)p\\left(c = 1\\right) + p\\left(\\textbf{x} \\mid c = 0\\right)p\\left(c = 0\\right)} \\\\\n",
    "&=& \\dfrac{1}{1 + e^{-a}} = \\sigma\\left(a\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\textbf{x}$ – вектор признаков объекта\n",
    "- $\\sigma$ – обозначение функции логистического сигмоида при скалярном аргументе\n",
    "- $a = \\log \\frac{p\\left(\\textbf{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\textbf{x} \\mid c = 0\\right)p\\left(c = 0\\right)} = \\sum_{i=0}^M w_i x_i$ – это отношение мы моделируем линейной функцией от признаков объекта и параметров модели\n",
    "\n",
    "Данное выражение легко обобщить до множества из $K$ классов, изменится только знаменатель в формуле Байеса. Запишем вероятность принадлежности объекта к классу $k$:\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = k \\mid \\textbf{x}\\right) &=& \\dfrac{p\\left(\\textbf{x} \\mid c = k\\right)p\\left(c = k\\right)}{\\sum_{i=1}^K p\\left(\\textbf{x} \\mid c = i\\right)p\\left(c = i\\right)} \\\\\n",
    "&=& \\dfrac{e^{z_k}}{\\sum_{i=1}^{K}e^{z_i}} = \\sigma_k\\left(\\textbf{z}\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\sigma_k$ – обозначение функции softmax при векторном аргументе\n",
    "- $z_k = \\log p\\left(\\textbf{x} \\mid c = k\\right)p\\left(c = k\\right) = \\sum_{i=0}^M w_{ki} x_i$ – это выражение моделируется линейной функцией от признаков объекта и параметров модели для класса $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для моделирования полного правдоподобия примера мы используем [категориальное распределение](https://en.wikipedia.org/wiki/Categorical_distribution), а лучше его логарифм (для удобства):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\mathcal{L} = \\log p\\left({\\textbf{x}, y}\\right) &=& \\log \\prod_{i=1}^K \\sigma_i\\left(\\textbf{z}\\right)^{y_i} \\\\\n",
    "&=& \\sum_{i=1}^K y_i \\log \\sigma_i\\left(\\textbf{z}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Получается хорошо знакомая нам функция [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) (если домножить на $-1$). Правдоподобие нужно максимизировать, а, соответственно, перекрестную энтропию нужно минимизировать. Продифференцировав по параметрам модели, мы _легко_ получим правила обновления весов для градиентного спуска, **проделайте этот вывод, если вы его не делали** (если вы вдруг сдались, то на [этом](https://www.youtube.com/watch?v=-WiR16raQf4) видео есть разбор вывода, понимание этого вам понадобится для дальнейшего выполнения задания; если предпочитаете текст, то и он есть [тут](https://www.ics.uci.edu/~pjsadows/notes.pdf) и [тут](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} &=& x_m \\left(y_k - \\sigma_k\\left(\\textbf{z}\\right)\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "В стандартной формулировке получается, что вектор $\\left(\\sigma_1, \\sigma_2, \\ldots, \\sigma_K\\right)$ образует дискретное вероятностное распределение, т.е. $\\sum_{i=1}^K \\sigma_i = 1$. Но в нашей постановке задачи каждый пример может иметь несколько тегов или одновременно принадлежать к нескольким классам. Для этого мы немного изменим модель:\n",
    "- будем считать, что все теги независимы друг от друга, т.е. каждый исход – это логистическая регрессия на два класса (либо есть тег, либо его нет), тогда вероятность наличия тега у примера запишется следующим образом (каждый тег/класс как и в многоклассовой логрегрессии имеет свой набор параметров):\n",
    "$$\\large p\\left(\\text{tag}_k \\mid \\textbf{x}\\right) = \\sigma\\left(z_k\\right) = \\sigma\\left(\\sum_{i=1}^M w_{ki} x^i \\right)$$\n",
    "- наличие каждого тега мы будем моделировать с помощью <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\">распределения Бернулли</a>\n",
    "\n",
    "<font color=\"red\">Вопрос 1.</font> Ваше первое задание –  записать упрощенное выражение логарифма правдоподобия примера с признаками $\\textbf{x}$. Как правило, многие алгоритмы оптимизации имеют интерфейс для минимизации функции, мы последуем этой же традиции и домножим полученное выражение на $-1$, а во второй части выведем формулы для минимизации полученного выражения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. $\\large -\\mathcal{L} = -\\sum_{i=1}^M y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "2. $\\large -\\mathcal{L} = -\\sum_{i=1}^K y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "3. $\\large -\\mathcal{L} = -\\sum_{i=1}^K z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$\n",
    "4. $\\large -\\mathcal{L} = -\\sum_{i=1}^M z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Вывод формулы обновления весов\n",
    "\n",
    "<font color=\"red\">Вопрос 2.</font> В качестве второго задания вам предоставляется возможность вывести формулу градиента для $-\\mathcal{L}$. Какой вид она будет иметь?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(\\sigma\\left(z_k\\right) - y_k\\right)$\n",
    "2. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(y_k - \\sigma\\left(z_k\\right)\\right)$\n",
    "3. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(\\sigma\\left(z_k\\right)x_m - y_k\\right)$\n",
    "4. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(y_k - \\sigma\\left(z_k\\right)x_m\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Реализация базовой модели\n",
    "\n",
    "Вам предлагается каркас класса модели, разберите его внимательно, обращайте внимание на комментарии. Затем заполните пропуски, запустите полученную модель и ответьте на проверочный вопрос.\n",
    "\n",
    "Как вы могли уже заметить, при обновлении веса $w_{km}$ используется значение признака $x_m$, который равен $0$, если слова с индексом $m$ нет в предложении, и больше нуля, если такое слово есть. В нашем случае, чтобы не пересчитывать [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) самим или с помощью [sklearn.feature_extraction.text.CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer), мы будем идти по словам предложения в порядке их следования. Если какое-то слово встречается несколько раз, то мы добавляем его в аккумулятор со своим весом. В итоге получится то же самое, как если сначала посчитать количество одинаковых слов и домножить на соответствующий вес. Соответственно, при вычислении линейной комбинации $z$ весов модели и признаков примера необходимо учитывать только ненулевые признаки объекта.\n",
    "\n",
    "Подсказка:\n",
    "- если реализовывать вычисление сигмоида так же, как в формуле, то при большом отрицательном значении $z$ вычисление $e^{-z}$ превратится в очень большое число, которое вылетит за допустимые пределы\n",
    "- в то же время $e^{-z}$ от большого положительного $z$ будет нулем\n",
    "- воспользуйтесь свойствами функции $\\sigma$ для того, чтобы пофиксить эту ошибку и реализовать $\\sigma$ без риска overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    if z > 36:\n",
    "                        z = 36\n",
    "                    elif z < -709:\n",
    "                        z = -709\n",
    "                        \n",
    "                    sigma = expit(z)\n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sample_loss += -y * np.log(sigma) - (1 - y) * np.log(1 - sigma)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = (y - sigma)\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c188c08c3d460ebe4e6e29ef851b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# создадим эксемпляр модели и пройдемся по датасету\n",
    "model = LogRegressor()\n",
    "model.iterate_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, действительно ли значение отрицательного логарифмического правдоподобия уменьшалось. Так как мы используем стохастический градентный спуск, не стоит ожидать плавного падения функции ошибки. Мы воспользуемся скользящим средним с окном в 10 тысяч примеров, чтобы хоть как-то сгладить график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nggM6Y4xZBAd0xhiziP8PU6fvNKjZEVQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the loss function on the last 10k train samples: 57.11\n"
     ]
    }
   ],
   "source": [
    "print('Mean of the loss function on the last 10k train samples: %0.2f' % np.mean(model._loss[-35000:-25000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 3.</font>\n",
    "Вычислите среднее значение функции стоимости на последних 10 000 примеров тренировочного набора, к какому из значений ваш ответ ближе всего?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 17.54\n",
    "2. 18.64\n",
    "3. 19.74\n",
    "4. 20.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Тестирование модели\n",
    "\n",
    "В базовой модели первые 100 000 строк используются для обучения, а оставшиеся – для тестирования. Как вы можете заметить, значение отрицательного логарифмического правдоподобия не очень информативно, хоть и позволяет сравнивать разные модели. В качестве четвертого задания вам необходимо модифицировать базовую модель таким образом, чтобы метод `iterate_file` возвращал значение _точности_ на тестовой части набора данных. \n",
    "\n",
    "Точность определим следующим образом:\n",
    "- считаем, что тег у вопроса присутствует, если спрогнозированная вероятность тега больше 0.9\n",
    "- точность одного примера расчитывается как [коэффициент Жаккара](https://ru.wikipedia.org/wiki/Коэффициент_Жаккара) между множеством настоящих тегов и предсказанных моделью\n",
    "  - например, если у примера настоящие теги ['html', 'jquery'], а по версии модели ['ios', 'html', 'java'], то коэффициент Жаккара будет равен |['html', 'jquery'] $\\cap$ ['ios', 'html', 'java']| / |['html', 'jquery'] $\\cup$ ['ios', 'html', 'java']| = |['html']| / |['jquery', 'ios', 'html', 'java']| = 1/4\n",
    "- метод `iterate_file` возвращает **среднюю** точность на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь\n",
    "class LogRegressor(LogRegressor):\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        acc = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                pred_tags = set()\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sigma = expit(z)\n",
    "                    \n",
    "                    if sigma == 1.0:\n",
    "                        sigma -= 10 ** -10\n",
    "                    elif sigma == .0:\n",
    "                        sigma += 10 ** -10\n",
    "    \n",
    "                    if n > top_n_train and sigma > 0.9:\n",
    "                        pred_tags.add(tag)\n",
    "        \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sample_loss += -y * np.log(sigma) - (1 - y) * np.log(1 - sigma)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = (y - sigma)\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "                acc += len(pred_tags & tags) / len(pred_tags | tags)\n",
    "        return acc / (n - top_n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "# выведем полученное значение с точностью до двух знаков\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 4.</font> К какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.39\n",
    "2. 0.49\n",
    "3. 0.59\n",
    "4. 0.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. $L_2$-регуляризация\n",
    "\n",
    "В качестве пятого задания вам необходимо добавить в класс `LogRegressor` поддержку $L_2$-регуляризации. В методе `iterate_file` должен появиться параметр `lmbda=0.01` со значением по умолчанию. С учетом регуляризации новая функция стоимости примет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(\\textbf W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\frac{\\lambda}{2} \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2\n",
    "\\end{array}$$\n",
    "\n",
    "Градиент первого члена суммы мы уже вывели, а для второго он имеет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial}{\\partial w_{ki}} \\frac{\\lambda}{2} R\\left(\\textbf W\\right) &=& \\lambda w_{ki}\n",
    "\\end{array}$$\n",
    "\n",
    "Если мы на каждом примере будем делать честное обновление всех весов, то все очень замедлится, ведь нам придется на каждой итерации пробегать по всем словам словаря. В ущерб теоретической корректности мы используем грязный трюк: будем регуляризировать только те слова, которые присутствуют в текущем предложении. Не забывайте, что смещение (bias) не регуляризируется. `sample_loss` тоже должен остаться без изменений.\n",
    "\n",
    "Замечание:\n",
    "- не забудьте, что нужно учитывать регуляризацию слова в градиентном шаге только один раз\n",
    "- условимся, что учитываем регуляризацию только при первой встрече слова\n",
    "- если бы мы считали сначала bag-of-words, то мы бы в цикле шли по уникальным словам, но т.к. мы этого не делаем, приходится выкручиваться (еще одна жертва богу online-моделей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь\n",
    "class LogRegressor(LogRegressor):\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16,\n",
    "                     lmbda = 0.01):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        acc = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                pred_tags = set()\n",
    "                used_word = set()\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sigma = expit(z)\n",
    "                    \n",
    "                    if sigma == 1.0:\n",
    "                        sigma -= 10 ** -10\n",
    "                    elif sigma == .0:\n",
    "                        sigma += 10 ** -10\n",
    "    \n",
    "                    if n > top_n_train and sigma > 0.9:\n",
    "                        pred_tags.add(tag)\n",
    "        \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sample_loss += -y * np.log(sigma) - (1 - y) * np.log(1 - sigma)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = (y - sigma)\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            if word not in used_word:\n",
    "                                used_word.add(word)    \n",
    "                                self._w[tag][self._vocab[word]] -= learning_rate * lmbda * self._w[tag][self._vocab[word]]\n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "                acc += len(pred_tags & tags) / len(pred_tags | tags)\n",
    "        return acc / (n - top_n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 5.</font> К какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.3\n",
    "2. 0.35\n",
    "3. 0.4\n",
    "4. 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ElasticNet регуляризация, вывод\n",
    "Помимо $L_2$ регуляризации, часто используется $L_1$ регуляризация.\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(\\textbf W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right|\n",
    "\\end{array}$$\n",
    "\n",
    "Если линейно объединить $L_1$ и $L_2$ регуляризацию, то полученный тип регуляризации называется ElasticNet:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\lambda R\\left(\\textbf W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\left(\\gamma \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2 + \\left(1 - \\gamma\\right) \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right| \\right)\n",
    "\\end{array}$$\n",
    "- где $\\gamma \\in \\left[0, 1\\right]$\n",
    "\n",
    "В качестве шестого вопроса вам предлагается вывести формулу градиента ElasticNet регуляризации (не учитывая $-\\mathcal{L}$). \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(\\textbf W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) w_{ki}\\right)$ \n",
    "2. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(\\textbf W\\right) = \\lambda \\left(2 \\gamma \\left|w_{ki}\\right| + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "3. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(\\textbf W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "4. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(\\textbf W\\right) = \\lambda \\left(\\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Регуляризация ElasticNet , реализация\n",
    "\n",
    "В качестве седьмой задачи вам предлается изменить класс `LogRegressor` таким образом, чтобы метод `iterate_file` принимал два параметра со значениями по умолчанию `lmbda=0.0002` и `gamma=0.1`. Сделайте один проход по датасету с включенной `ElasticNet`-регуляризацией и заданными значениями по умолчанию и ответьте на вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 7.</font> К какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.59\n",
    "2. 0.69\n",
    "3. 0.79\n",
    "4. 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Самые важные слова для тега\n",
    "\n",
    "Прелесть линейных моделей в том, что они легко интерпретируемы. Вам предлагается вычислить, какие слова вносят наибольший вклад в вероятность появления каждого из тегов. А затем ответьте на контрольный вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model._vocab_inv = dict([(v, k) for (k, v) in model._vocab.items()])\n",
    "\n",
    "for tag in model._tags:\n",
    "    print(tag, ':', ', '.join([model._vocab_inv[k] for (k, v) in \n",
    "                               sorted(model._w[tag].items(), \n",
    "                                      key=lambda t: t[1], \n",
    "                                      reverse=True)[:5]]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 8.</font> Для многих тегов наличие самого тега в предложении является важным сигналом, у многих сам тег является самым сильным сигналом, что не удивительно. Для каких из тегов само название тега не входит в топ-5 самых важных?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. c# \n",
    "2. javascript\n",
    "3. jquery\n",
    "4. android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. Сокращаем размер словаря\n",
    "Сейчас количество слов в словаре – 519290, если бы это была выборка из 10 миллионов вопросов с сайта StackOverflow, то размер словаря был бы миллионов 10. Регуляризировать модель можно не только изящно математически, но и топорно, например, ограничить размер словаря. Вам предоставляется возможность внести следующие изменения в класс `LogRegressor`:\n",
    "- добавить в метод `iterate_file` еще один аргумент со значением по умолчанию `update_vocab=True`\n",
    "- при `update_vocab=True` разрешать добавлять слова в словарь в режиме обучения\n",
    "- при `update_vocab=False` игнорировать слова не из словаря\n",
    "- добавить в класс метод `filter_vocab(n=10000)`, который оставит в словаре только топ-n самых популярных слов, используя данные из ``train``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# оставим только топ 10 000 слов\n",
    "model.filter_vocab(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# сделаем еще одну итерацию по датасету, уменьшив скорость обучения в 10 раз\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 9.</font> К какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.48\n",
    "2. 0.58\n",
    "3. 0.68\n",
    "4. 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Прогнозирование тегов для новых вопросов\n",
    "\n",
    "В завершение этого задания вам предлагается реализовать метод `predict_proba`, который принимает строку, содержащую вопрос, а возвращает список предсказанных тегов вопроса с их вероятностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "model.filter_vocab(n=10000)\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = (\"I want to improve my coding skills, so I have planned write \" +\n",
    "            \"a Mobile Application.need to choose between Apple's iOS or Google's Android.\" +\n",
    "            \" my background: I have done basic programming in .Net,C/C++,Python and PHP \" +\n",
    "            \"in college, so got OOP concepts covered. about my skill level, I just know \" +\n",
    "            \"concepts and basic syntax. But can't write complex applications, if asked :(\" +\n",
    "            \" So decided to hone my skills, And I wanted to know which is easier to \" +\n",
    "            \"learn for a programming n00b. A) iOS which uses Objective C B) Android \" + \n",
    "            \"which uses Java. I want to decide based on difficulty \" + \n",
    "            \"level\").lower().replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(model.predict_proba(sentence).items(), \n",
    "       key=lambda t: t[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 10.</font> Отметьте все теги, ассоциирующиеся с данным вопросом, если порог принятия равен $0.9$. То есть считаем, что вопросу надо поставить некоторый тег, если вероятность его появления, предсказанная моделью, больше или равна 0.9. \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. android\n",
    "2. ios\n",
    "3. php\n",
    "4. java"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
